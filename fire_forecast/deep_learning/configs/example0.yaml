training:
    learning_rate: 0.001
    learning_rate_development:
        type: exponential # or exponential (decrease by factor every time)
        amount: 0.99 # if linear, decrease by this amount every time if exponential, decrease by this factor every time
    batch_size: 128
    epochs: 2
    optimizer_name: Adam
    optimizer_args: {} # will take learning rate from above
    loss_function: WeightedL1Loss    

output:
    path: /data/runs/test # where to save the model
    checkpoint_interval: 1 # save a checkpoint every 15 epochs

data:
    train_path: /data/deep_learning_data/GFAS_all_data/no_shift/train.hdf
    test_path: /data/deep_learning_data/GFAS_all_data/no_shift/test.hdf
    validation_path: /data/deep_learning_data/GFAS_all_data/no_shift/validation.hdf
    variables: null # is filled automatically by the iterator

model:
    name: FullyConnectedForecaster
    model_args: 
        input_size: 3024
        output_size: 24
        hidden_layer_sizes: [2048, 1024, 1024, 512, 512, 256, 256, 128 ,128, 64, 32]
        dropout: 0.05
        batch_norm: false
        activation: LeakyReLU
        final_activation: null
    checkpoint_path: null #/home/tm/Data/test/checkpoint_30.pt #null # if you want to load a checkpoint

